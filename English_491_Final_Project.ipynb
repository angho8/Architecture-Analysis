{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6gUqoA3mlJuImj7dm//jx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angho8/Architecture-Analysis/blob/main/English_491_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0djNGyv5tI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "33fc313e-032d-422d-cbf0-91ea597772b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-674d6d68a225>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Install necessary libraries if they're not already available in your Colab environment\n",
        "!pip install requests beautifulsoup4 nltk spacy sklearn\n",
        "\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import files\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "1DGrz7z3JKyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read content from a .txt file\n",
        "def read_text_from_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "nlPjuHwVIZoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process and analyze the text\n",
        "def analyze_text(text):\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase and filter out non-alphabetic words and stopwords\n",
        "    words = [word.lower() for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Frequency distribution of words\n",
        "    freq_dist = FreqDist(filtered_words)\n",
        "\n",
        "    # Sentiment analysis using VADER\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_score = sia.polarity_scores(text)\n",
        "\n",
        "    return freq_dist, sentiment_score"
      ],
      "metadata": {
        "id": "wOsNVjN7IZav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition (NER) to extract key architectural entities\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {\"architects\": [], \"buildings\": [], \"periods\": []}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            entities[\"architects\"].append(ent.text)\n",
        "        elif ent.label_ == \"FAC\":  # Facility, usually building names\n",
        "            entities[\"buildings\"].append(ent.text)\n",
        "        elif ent.label_ == \"TIME\":  # Time period\n",
        "            entities[\"periods\"].append(ent.text)\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "ZF0XJJa4JULw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine similarity to measure the similarity between two articles\n",
        "def calculate_similarity(texts):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
        "\n",
        "    return cosine_sim\n"
      ],
      "metadata": {
        "id": "u49WU4QvJWGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic modeling using LDA (Latent Dirichlet Allocation)\n",
        "def perform_topic_modeling(texts, num_topics=5):\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    processed_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "        processed_texts.append(tokens)\n",
        "\n",
        "    # Create a dictionary and corpus\n",
        "    dictionary = corpora.Dictionary(processed_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "    # Apply LDA\n",
        "    lda_model = gensim.models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "    topics = lda_model.print_topics(num_words=5)\n",
        "\n",
        "    return topics"
      ],
      "metadata": {
        "id": "dLXd9sQIJ3UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write results to a CSV file\n",
        "def write_results_to_csv(results, filename='output_results.csv'):\n",
        "    # Define CSV headers\n",
        "    headers = ['Filename', 'Most Common Words', 'Sentiment', 'Architects', 'Buildings', 'Periods', 'Topics']\n",
        "\n",
        "    # Write to CSV file\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for result in results:\n",
        "            writer.writerow(result)\n",
        "\n",
        "    print(f\"Results saved to {filename}\")"
      ],
      "metadata": {
        "id": "BmFWpVHJKe19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to process multiple text files\n",
        "def main():\n",
        "    # Upload .txt files for offline analysis\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # List to store articles from .txt files\n",
        "    articles = []\n",
        "\n",
        "    # Process uploaded .txt files\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"Processing file: {filename}...\")\n",
        "\n",
        "        # Read the text from the file\n",
        "        text = read_text_from_file(filename)\n",
        "\n",
        "        if text:\n",
        "            articles.append(text)\n",
        "            print(f\"Analyzing text from {filename}...\")\n",
        "            freq_dist, sentiment_score = analyze_text(text)\n",
        "            print(f\"Most common words in {filename}: {freq_dist.most_common(10)}\")\n",
        "            print(f\"Sentiment analysis for {filename}: {sentiment_score}\")\n",
        "            entities = extract_entities(text)\n",
        "            print(f\"Extracted entities from {filename}: {entities}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    # Compare similarities between the articles\n",
        "    compare_articles(articles)\n",
        "\n",
        "    # Perform topic modeling to discover themes in the articles\n",
        "    topics = perform_topic_modeling(articles)\n",
        "    print(\"Discovered Topics from the Articles:\")\n",
        "    for idx, topic in enumerate(topics):\n",
        "        print(f\"Topic {idx + 1}: {topic}\")"
      ],
      "metadata": {
        "id": "epUo9Frp6IFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compare multiple articles\n",
        "def compare_articles(articles):\n",
        "    print(\"Comparing articles for architectural similarities...\")\n",
        "    similarities = calculate_similarity(articles)\n",
        "\n",
        "    for i in range(len(articles)):\n",
        "        print(f\"Similarity with article {i + 1}: {similarities[0][i]}\")\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "EtXWVNI8JZHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}